{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Stream","text":"<p>Stream is a design space exploration (DSE) framework for mapping deep neural networks (DNNs) onto multi-core heterogeneous dataflow accelerators. It supports a wide spectrum of architectural designs and scheduling granularities\u2014from traditional layer-by-layer execution to advanced layer-fused processing\u2014enabling scalable, efficient deployment of modern DNNs.</p> <p>Stream builds upon the ZigZag framework but significantly extends its capabilities to multi-core and fine-grained scheduling contexts.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>Layer Fusion Support   Enables splitting and scheduling parts of layers across multiple cores for higher utilization and lower memory access costs.</p> </li> <li> <p>Heterogeneous Multi-Core Scheduling   Models realistic accelerator architectures including cores with different compute/memory capabilities and interconnects.</p> </li> <li> <p>Memory &amp; Communication-Aware Analysis (COALA)   Stream integrates COALA: a validated latency and energy model that captures data reuse, communication overhead, and memory hierarchies.</p> </li> <li> <p>Workload Allocation via Constraint Optimization (WACO)   Stream includes a built-in engine that explores valid allocations across cores using constraint-based optimization.</p> </li> <li> <p>Validated Against Real Hardware   Performance models and predictions are benchmarked against three state-of-the-art accelerator designs.</p> </li> <li> <p>Modular &amp; Extensible   Stages of the mapping process are customizable, enabling easy experimentation and research integration.</p> </li> </ul>"},{"location":"#get-started","title":"\ud83d\ude80 Get Started","text":"<ol> <li>Clone and install requirements</li> </ol> <pre><code>git clone https://github.com/KULeuven-MICAS/stream.git\ncd stream\npip install -r requirements.txt\n</code></pre> <ol> <li>Try the tutorial</li> </ol> <pre><code>git checkout tutorial\npython lab1/main.py\n</code></pre> <p>More step-by-step setup help can be found in the Getting Started and Installation pages.</p>"},{"location":"#publications","title":"\ud83d\udcda Publications","text":"<p>The framework and methodology are described in:</p> <p>A. Symons, L. Mei, S. Colleman, P. Houshmand, S. Karl and M. Verhelst, \u201cStream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators\u201d, IEEE Transactions on Computers, 2025. \ud83d\udcc4 Read our paper</p> <p>Stream enables researchers and developers to design, evaluate, and optimize novel DNN hardware accelerators \u2014 particularly for latency-sensitive, power-constrained edge applications.</p> <p>Happy exploring!</p>"},{"location":"contribute/","title":"Contribute","text":""},{"location":"contribute/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>When contributing to the framework, please follow these guidelines:</p> <ul> <li>Use Google's Python Style Guide</li> <li>Use Google-style docstrings for classes, functions, and methods.   See examples here</li> <li>Update the documentation when you change or add public functionality</li> </ul>"},{"location":"contribute/#quick-setup","title":"Quick Setup","text":"<ol> <li>Clone &amp; create a branch</li> </ol> <pre><code>git clone &lt;your-fork-url&gt;\ncd stream\ngit checkout -b &lt;feature-or-fix&gt;\n</code></pre> <ol> <li>Install the dev tools (one-time only)</li> </ol> <pre><code># Python \u2265 3.11\npython -m venv .venv\nsource .venv/bin/activate\npip install -U pip ruff pre-commit pytest\npre-commit install  # hooks run on every commit\n</code></pre> <ol> <li> <p>Use VS Code with Ruff</p> </li> <li> <p>Install the Ruff extension (<code>charliermarsh.ruff</code>)</p> </li> <li>Enable Format on Save</li> <li> <p>Disable other formatters/linters to avoid conflicts</p> </li> <li> <p>Run the full check suite</p> </li> </ol> <pre><code>ruff check .       # lint + auto-fix suggestions\nruff format .      # apply formatting\npytest             # run tests\n</code></pre>"},{"location":"contribute/#coding-style","title":"Coding Style","text":"<ul> <li>Style guide \u2013 Google Python Style Guide</li> <li>Line length \u2013 120 characters (enforced by Ruff)</li> <li>Docstrings \u2013 Google-style   (see example)</li> <li>Type hints \u2013 Required for all public functions, classes, and methods</li> </ul>"},{"location":"contribute/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li>Ensure all checks pass:</li> </ol> <pre><code>ruff check .\nruff format --check .\npytest\n</code></pre> <ol> <li>Add or update unit tests  </li> <li>Update documentation if public APIs change  </li> <li>Open a pull request and fill out the PR template</li> </ol> <p>Thanks for contributing to Stream!</p>"},{"location":"future/","title":"Future changes","text":"<p>Here you can find the future changes we are planning on making to both the framework and the documentation.</p>"},{"location":"future/#framework","title":"Framework","text":"<ul> <li>Exploit cyclic behavior of steady-state portion of graph schedule to     speed up scheduling.</li> </ul>"},{"location":"future/#documentation","title":"Documentation","text":"<ul> <li>Add Doxygen code documentation.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#tutorial","title":"Tutorial","text":"<p>The recommended way to get started with Stream is through the tutorial labs. You can find them in the <code>tutorial</code> branch of the repository. You should start with lab1 here.</p>"},{"location":"getting-started/#manual-run","title":"Manual run","text":"<p>You can also run the main script directly. This will run a genetic algorithm on the inputs defined in the main file. You can read the file for more details.</p> <pre><code>$ python main_stream_ga.py\n</code></pre>"},{"location":"hardware/","title":"Hardware Architecture","text":""},{"location":"hardware/#hw-accelerator-model","title":"HW Accelerator Model","text":"<p>Multiple cores are combined together into the Stream accelerator object.</p> <p></p> <p>The Stream accelerator definition includes:</p> <ul> <li>name: A user-defined name for the accelerator.</li> <li>cores: A list of cores, each defined in a .yaml file. Absolute     or relative paths are allowed. The standard path assumed is     <code>\"./cores/\"</code>.</li> <li>graph: Definition of the core interconnection topology.</li> </ul>"},{"location":"hardware/#graph-definition","title":"Graph Definition","text":"<p>The graph definition determines the core interconnection topology. Currently, two topologies are supported: '2d-mesh' and 'bus'.</p> <p>The '2d-mesh' topology is defined through the following fields:</p> <ul> <li>type: The type of the graph, which should be '2d-mesh'.</li> <li>nb_rows: The number of rows in the 2D mesh.</li> <li>nb_cols: The number of columns in the 2D mesh.</li> <li>bandwidth: The bandwidth of each created directional link in     bits per clock cycle.</li> <li>unit_energy_cost: The unit energy cost of having a     communication-link active. This does not include the involved memory     read/writes.</li> <li>pooling_core_id: If provided, the pooling core id. A link is     added between the pooling core and each compute core in the mesh.</li> <li>simd_core_id: If provided, the simd core id. A link is added     between the simd core and each compute core in the mesh.</li> <li>offchip_core_id: If provided, the offchip core id. A link is     added between the offchip core and each compute core in the mesh.</li> </ul> <p>The 'bus' topology is defined through the following fields:</p> <ul> <li>type: The type of the graph, which should be 'bus'.</li> <li>bandwidth: The bandwidth of the bus in bits per clock cycle.</li> <li>unit_energy_cost: The unit energy cost of having a     communication-link active. This does not include the involved memory     read/writes.</li> <li>pooling_core_id: If provided, the pooling core id. A link is     added between the pooling core and each compute core in the mesh.</li> <li>simd_core_id: If provided, the simd core id. A link is added     between the simd core and each compute core in the mesh.</li> <li>offchip_core_id: If provided, the offchip core id. A link is     added between the offchip core and each compute core in the mesh.</li> </ul>"},{"location":"hardware/#modelled-examples","title":"Modelled examples","text":"<p>Several examples about how to model the hardware architectures in Stream can be found here.</p>"},{"location":"installation/","title":"Installing Stream","text":""},{"location":"installation/#manual-clone","title":"Manual clone","text":"<p>If you want to add custom functionality to the framework, you can clone the repository manually:</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>git</code>: for cloning the repository</li> <li><code>pip</code>: for installing the required packages</li> <li><code>python&gt;=3.8</code>: for running the framework</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>Clone the repository</p> <pre><code>git clone git@github.com:KULeuven-MICAS/stream.git\n</code></pre> <p>or</p> <pre><code>git clone https://github.com/KULeuven-MICAS/stream.git\n</code></pre> <p>Install requirements through pip. Alternatively, anaconda spec file is also provided.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Once the dependencies are installed, Stream can be run through the main file. More details are provided in Getting Started.</p>"},{"location":"mapping/","title":"Mapping","text":"<p>Stream requires two different specification about the possible mapping of a workload to a hardware architecture. These two mapping specification are</p> <ol> <li>the spatial mapping of each core (defined as the 'dataflow' in     <code>hardware</code>)</li> <li>the possible core allocation of each layer type (defined in a     file in the mapping     folder)</li> </ol> <p>These two specifications will be further explained on this page:</p>"},{"location":"mapping/#spatial-mapping","title":"Spatial Mapping","text":"<p>The spatial mapping describes the spatial parallelization strategy used in a certain core. The spatial mapping has to be specified in the hardware architecture as an attribute to each core (see explanation here and example here). An example dataflow could look like:</p> <pre><code>[{\"D1\": (\"K\", 16), \"D2\": (\"C\", 16)}]\n</code></pre> <p>In this example the Operational Array has two dimensions (i.e. D1 and D2). The output channels (\"K\") are unrolled over D1 and the input channels (\"C\") are unrolled over D2. Both dimensions have an unrolling factor of 16.</p>"},{"location":"mapping/#core-allocation","title":"Core Allocation","text":"<p>Besides the spatial mapping, the user has to provide information about which layer type can be exectued on which core in the hardware architecture. An example core allocation for the architecture here could look like:</p> <pre><code>mapping = {\n    \"/conv1/Conv\": {\n        \"core_allocation\": 2  # or (2,)\n    },\n    \"/conv2/Conv\": {\n        \"core_allocation\": (0, 1, 2, 3)\n    },\n    \"pooling\": {\n        \"core_allocation\": 4,\n    },\n    \"simd\": {\n        \"core_allocation\": 5,\n    }\n    \"default\": {\n        \"core_allocation\": [0, 1, 2, 3],\n    },\n}\n</code></pre> <p>In this example:</p> <ol> <li>The layer with name \"/conv1/Conv\" will have a fixed core     allocation onto core 2.</li> <li>The layer with name \"/conv2/Conv\" will have a fixed core     allocation for its groups (see [IntraCoreMappingStage]{.title-ref}     in Stages for more information     regarding groups).</li> <li>All layers of type \"pooling\" will be allocated to core 4.</li> <li>All layers of type \"simd\" (case insensitive) will be allocated to     core 5.</li> <li>All other layers can be allocated to cores 0, through 3 by default.</li> </ol> <p>When determining the possible core allocations for a node, the name is checked first, then the type, then the default is used as a last resort. The available layer types are all the ones introduced in Workload.</p>"},{"location":"mapping/#saving-an-scmes-allocation","title":"Saving an SCME's allocation","text":"<p>When you have run Stream for optimizing a layer-core allocation, it can be interesting to save the obtained allocation for future use as a fixed mapping, without having to re-run the genetic algorithm. The obtained allocation can be saved to a python file through use of the save_core_allocation function. The code below demonstrates its use:</p> <pre><code>from pprint import pprint\nfrom stream.utils import load_scme, save_core_allocation\n\nscme_path = 'my/saved.scme'\nscme = load_scme(scme_path)\nd = save_core_allocation(scme.workload, \"my/fixed/mapping.py\")\npprint(d)\n</code></pre>"},{"location":"outputs/","title":"Outputs","text":"<p>After the execution of all the stages has finished (<code>scme, _ = mainstage.run()</code>), the user has access to a variety of optimal Stream Cost Model Evaluations (SCMEs) produced by the genetic algortihm of Stream (i.e. Hall of fame). Each of these SCMEs offers a layer-core allocation with an unique trade-off due to the usage of a NSGA-II genetic algorithm.</p>"},{"location":"outputs/#printing-the-attributs-of-a-specific-scme","title":"Printing the attributs of a specific SCME","text":"<p>The user can select one of these SCMEs by using selecting a specific SCME with the command <code>scme = scme[0]</code>. Another SCME can be selected by using a different index than <code>0</code>.</p> <p>After selecting a SCME with the previous command, the attributes of the SCME can be printed to the terminal with the following instructions:</p> <pre><code>from pprint import pprint\npprint(vars(scme))\n</code></pre> <p>After a specific SCME is selected, the user can generate a variety of outputs which will be introduced in the following. If you are using the example provided in this file, then all the introduced outputs are saved in the <code>outputs</code> folder in your repo.</p>"},{"location":"outputs/#pickle-file-of-scme","title":"Pickle file of SCME","text":"<p>By using the <code>save_scme()</code> function from <code>stream.utils</code>, the user can save a specific SCME in a pickle file for later investigations. The SCME includes all attributes of the <code>StreamCostModelEvaluation</code> in the cost_model.py file. Examples for these attributes are the latency or the maximal memory usage of layer-core allocation underling to the SCME.</p>"},{"location":"outputs/#schedule-visualization","title":"Schedule visualization","text":"<p>Perfetto visualization ====================</p> <p>The schedule of a specific SCME can be saved in a Perfetto json file by calling <code>convert_scme_to_perfetto_json()</code> from <code>stream.visualization.perfetto</code>. The saved file can be opened at https://ui.perfetto.dev and the different parts of the schedule can be further investigated.</p>"},{"location":"outputs/#png-visualization","title":"PNG visualization","text":"<p>By using the <code>plot_timeline_brokenaxes()</code> function from <code>stream.visualization.schedule</code>, a portion of the entire schedule can be visualized to a <code>png</code> file.</p>"},{"location":"outputs/#memory-usage-visualization","title":"Memory usage visualization","text":"<p>The function <code>plot_memory_usage()</code> from <code>stream.visualization.memory_usage</code> allows to save a visualization of the memory usage of a specific SCME in a <code>png</code> file. An example can look like the following diagram:</p> <p>{width=\"900px\"}</p> <p>The diagram shows the utilization of the on-chip SRAM memories of the different cores in the hardware architecture for a specific SCME. In this example each core of core 0 to core 3 has two SRAM memories. One of the two SRAM memories (e.g. <code>sram_1MB_W</code>) holds the weights for the processing in the core. The other SRAM memory of each core (e.g. <code>sram_1MB_A</code>) holds the activations (i.e. input and output activations) of the compuations in the core. In this example, core 4 and core 5 have only one memory to store all relevant data. The plots for each of these memories show how much they are utilized during the processing of the workload.</p>"},{"location":"publications/","title":"Publications","text":"<p>Here are the pointers to Stream-project publications. Stream is an extension to the ZigZag project.</p>"},{"location":"publications/#the-general-idea-of-zigzag","title":"The general idea of ZigZag","text":"<ul> <li> <p>L. Mei, P. Houshmand, V. Jain, S. Giraldo and M. Verhelst, \"ZigZag:     Enlarging Joint Architecture-Mapping Design Space Exploration for     DNN Accelerators,\" in IEEE Transactions on Computers, vol. 70, no.     8, pp. 1160-1174, 1 Aug. 2021.</p> <p>[paper]</p> </li> </ul>"},{"location":"publications/#extension-to-support-multi-core-layer-fused-scheduling-stream","title":"Extension to support multi-core layer-fused scheduling (Stream)","text":"<ul> <li> <p>A. Symons, L. Mei, S. Colleman, P. Houshmand, S. Karl and M.     Verhelst, \"Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators\",     Transactions on Computers, vol. 74, no. 1, pp. 237-249, Jan. 2025.</p> <p>[paper], [github]</p> </li> <li> <p>S. Karl, A. Symons, N. Fasfous and M. Verhelst, \"Genetic     Algorithm-based Framework for Layer-Fused Scheduling of Multiple     DNNs on Multi-core Systems,\" 2023 Design, Automation &amp; Test in     Europe Conference &amp; Exhibition (DATE), Antwerp, Belgium, 2023, pp.     1-6, doi: 10.23919/DATE56975.2023.10137070.</p> <p>[paper], [slides], [video]</p> </li> </ul>"},{"location":"stages/","title":"Stages","text":"<p>This document explains the concept of stages within the Stream framework. It details the different implemented stages and explains how to create your own.</p>"},{"location":"stages/#introduction","title":"Introduction","text":"<p>Stages in Stream allow modular customization of the framework\u2019s behavior. The sequence of stages defines what the framework will execute. These are configured in the <code>MainStage</code>. Example:</p> <pre><code>mainstage = MainStage(\n    [\n        AcceleratorParserStage,\n        StreamONNXModelParserStage,\n        LayerSplittingStage,\n        StreamONNXModelParserStage,\n        GenerateCNWorkloadHybridStage,\n        IntraCoreMappingStage,\n        InterCoreMappingStage,\n    ],\n    accelerator=accelerator,\n    workload_path=workload_path,\n    mapping_path=mapping_path,\n    loma_lpf_limit=6,\n    nb_ga_individuals=32,\n    nb_ga_generations=100,\n    cost_lut_path=cost_lut_path,\n    plot_hof=True,\n    plot_file_name=plot_file_name,\n    plot_full_schedule=plot_full_schedule,\n    plot_data_transfer=plot_data_transfer,\n    cn_define_mode=CN_define_mode,\n    hint_loops=hint_loops,\n    scheduler_candidate_selection=\"memory\",\n    operands_to_prefetch=[],\n    split_onnx_model_path=split_onnx_model_path,\n    split_W_double_buffered=split_W_double_buffered,\n)\n\nscme, _ = mainstage.run()\nscme = scme[0]\n</code></pre>"},{"location":"stages/#implemented-stages","title":"Implemented Stages","text":"<p>See the stream/stages/ folder for up-to-date source definitions.</p>"},{"location":"stages/#customspatialmappinggeneratorstage","title":"CustomSpatialMappingGeneratorStage","text":"<p>Finds spatial mappings given an accelerator, core allocation, and interconnection pattern. Uses the innermost memory levels to determine dataflow.</p>"},{"location":"stages/#generatecnworkloadhybridstage","title":"GenerateCNWorkloadHybridStage","text":"<p>Transforms a layer-by-layer workload into a fine-grained CN workload graph. Configurable via <code>cn_define_mode</code> and <code>hint_loops</code>.</p> <p>Modes include:</p> <ol> <li><code>hint_loops</code> defines outer-cn loops for splitting.</li> <li><code>hint_loops</code> defines inner-cn loops; all others become outer-cn.</li> <li><code>hint_loops</code> is a list-of-lists; <code>layer_cutoffs</code> defines to which layers each applies.</li> <li><code>hint_loops</code> defines outer-cn; <code>split_W_percentage</code> limits constant operand memory footprint. Layers exceeding the limit are split in K.</li> </ol>"},{"location":"stages/#intercoremappingstage","title":"InterCoreMappingStage","text":"<p>Performs inter-core mapping using a genetic algorithm. Starts from CMEs collected during intra-core mapping.</p>"},{"location":"stages/#intracoremappingstage","title":"IntraCoreMappingStage","text":"<p>Finds optimal CMEs per node-core allocation. Groups nodes based on <code>loop_ranges</code> differences in relevant dimensions (e.g., <code>K</code> for convolution).</p>"},{"location":"stages/#onnxmodelparserstage","title":"ONNXModelParserStage","text":"<p>Parses a workload file into a <code>NetworkX</code> graph. Converts ONNX into Stream\u2019s internal representation.</p> <p>You can also reuse ZigZag\u2019s implemented stages within Stream.</p>"},{"location":"stages/#creating-a-custom-stage","title":"Creating a Custom Stage","text":"<p>To create a custom stage (e.g., optimize for something other than energy), copy an existing stage and modify it as needed. Make sure to:</p> <ul> <li>Inherit from the abstract <code>Stage</code> class</li> <li>Initialize your substage at the beginning of your callables list:   <pre><code>substages = [YourNextStage(...), ...]\nstage = YourStage(substages, **kwargs)\n</code></pre></li> <li>Iterate over <code>for cme, extra_info in substage.run():</code> to yield results</li> <li>If you're reducing (like <code>MinimalLatencyStage</code>), yield outside the loop</li> </ul> <p>Creating custom stages enables targeted optimization and full control over the pipeline logic.</p>"},{"location":"user-guide/","title":"User Guide","text":"<p>The Stream framework consists of five major building blocks. The documents listed below explain each component in detail.</p> <p>If you're looking for implementation details or want to understand the design philosophy behind Stream, refer to the publications page or dive into the code on GitHub.</p> <p></p>"},{"location":"user-guide/#components","title":"Components","text":"<ul> <li>Workload: Learn how Stream interprets deep learning workloads and represents them as computation graphs.</li> <li>Hardware: Explore how hardware architectures are modeled and parsed into the framework.</li> <li>Mapping: Understand the process of mapping workloads to multi-core accelerators, including both intra- and inter-core strategies.</li> <li>Stages: A modular pipeline structure lets you configure or extend Stream\u2019s behavior with custom stages.</li> <li>Outputs: Discover what results Stream produces\u2014visualizations, performance metrics, and more.</li> </ul>"},{"location":"workload/","title":"Workload","text":"<p>The recommended way of defining an algorithmic workload is through an onnx model. An onnx model can contain multiple operator types, which in the context of ML are often referred to as layers, some of which are automatically recognised and parsed by Stream. Alternatively, the layers can be manually defined for more customization.</p> <p>Stream handles workloads in a similar way than the original ZigZag project. Additionally, Stream supports more onxx operators, but it also requires slightly different layer attributes in the manual layer definition.</p>"},{"location":"workload/#onnx-models","title":"ONNX models","text":""},{"location":"workload/#supported-operators","title":"Supported operators","text":"<p>A complete list of all onnx operators can be found here.</p> <p>Following operators are supported by Stream and will automatically be parsed into <code>LayerNode</code> objects when using your onnx model within the framework:</p> <ul> <li>Conv</li> <li>QLinearConv</li> <li>MatMul</li> <li>Gemm</li> <li>MaxPool</li> <li>AveragePool</li> <li>GlobalMaxPool</li> <li>GlobalAveragePool</li> <li>Reshape</li> <li>Flatten</li> <li>Add</li> <li>Mul</li> <li>Transpose</li> <li>LpNormalization</li> </ul> <p>All other operators will be parsed into a <code>DummyNode</code> object, which is assumed to not be accelerateable, incurring 0 hardware cost. If you have an onnx operator you would like to see supported, feel free to open an issue or manually add it yourself in the ONNXModelParserStage taking into account the Contributing Guidelines.</p>"},{"location":"workload/#saving-weights-as-external-data","title":"Saving weights as external data","text":"<p>If your onnx model is rather large, and you want to avoid having it inside of your Stream repo, you can save it with external data, which saves the weights as an external file, which can be discarded as Stream doesn't require the weight values. You can do so as follows:</p> <pre><code>import onnx\nmodel = onnx.load('my_model_with_internal_data.onnx')\nonnx.save_model(\n    model,\n    'my_model_with_external_data.onnx',\n    save_as_external_data=True,\n    all_tensors_to_one_file=True,\n    location='external_data.data',\n    size_threshold=1024,\n    convert_attribute=False\n)\n# Optional: Remove the 'external_data_filename' file if you don't need the data\n# When loading model, add extra flag as follows:\nmodel = onnx.load_model('my_model_with_internal_data.onnx', load_external_data=False)\n</code></pre>"},{"location":"workload/#shape-inference","title":"Shape inference","text":"<p>Stream requires an inferred onnx model, as it needs to know the shapes of all intermediate tensors to correctly infer the layer shapes. If you have an onnx model that is not shape inferred, you can do so by the following commands:</p> <pre><code>import onnx\nfrom onnx import shape_inference\nmodel = onnx.load(\"my_model.onnx\")\ninferred_model = shape_inference.infer_shapes(model)\nonnx.save(inferred_model, \"my_inferred_model.onnx\")\n</code></pre>"},{"location":"workload/#manual-layer-definition","title":"Manual layer definition","text":"<p>It is also possible to manually define your own workload layers. In that case there the <code>main.py</code> file should be executed instead of <code>main_onnx.py</code>. Moreover, the workload file should be provided as input together with the accelerator, thus there is no onnx model and mapping file loaded. The mapping information is inserted for each layer alongside the layer shape definition, identically to how it was defined in the mapping file.</p> <p>Each layer definition is represented as a dict which should have the following attributes:</p> <ul> <li>operator_type: Specification of the layer operator type (e.g.     <code>Conv</code>, <code>MaxPool</code>).</li> <li>equation: The operational equation for this layer. The     dimensions should be small letters, where as the operands are large     letters. 'O' should always be used for the output operand, the     input operands can be named freely.</li> <li>dimension_relations: The relationship between different     dimensions present in the equation. This is often used in     convolutional layers, where there is a relationship between the     spatial input indices and the spatial output indices through the     stride and with the filter indices through the dilation rate.</li> <li>loop_dim_size: The size of the different dimensions present in     the equation. Dimensions defined (i.e. on the left hand side) in the     dimension_relations are not to be provided and are inferred     automatically.</li> <li>operand_precision: The bit precision of the different operands     present in the equation. 'O' should always be used, which     represents the partial output precision. 'O_final' represents the     final output precision.</li> <li>operand_source: The layer id the input operands of this layer     come from. This is important to correctly build the NN graph edges.</li> <li>memory_operand_links: The link between the virtual memory     operands and the actual algorithmic operands. For more information,     read the hardware readme.</li> <li>padding: Padding for each dimension (i.e. IX and IY). The first     value is the left or upper padding, the second value is the right or     lower padding for IX resp. IY</li> </ul> <p>The following loop notation has to be used to describe a layer of the workload (see loop notation in this paper):</p> <ul> <li>B: Batch size</li> <li>K: Output channels</li> <li>C: Input channels</li> <li>OY: Output rows</li> <li>OX: Output columns</li> <li>FY: Kernel rows</li> <li>FX: Kernel columns</li> </ul> <p>An example of this manual layer defintion can be found at: inputs/examples/workloads/resnet18.yaml.</p>"}]}